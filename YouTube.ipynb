{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c35ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43092 entries, 0 to 43091\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   header            43092 non-null  object\n",
      " 1   title             43092 non-null  object\n",
      " 2   titleUrl          42323 non-null  object\n",
      " 3   subtitles         38250 non-null  object\n",
      " 4   time              43092 non-null  object\n",
      " 5   products          43092 non-null  object\n",
      " 6   activityControls  43092 non-null  object\n",
      " 7   details           1059 non-null   object\n",
      " 8   description       479 non-null    object\n",
      "dtypes: object(9)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# specify the path to the JSON file\n",
    "json_file_path = r\"C:\\Users\\ashka\\Downloads\\takeout-20230509T172344Z-001\\Takeout\\YouTube and YouTube Music\\history\\watch-history.json\"\n",
    "\n",
    "# read the JSON file into a pandas dataframe\n",
    "df_watch_history = pd.read_json(json_file_path)\n",
    "\n",
    "# display the dataframe\n",
    "df_watch_history.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec9830f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>title</th>\n",
       "      <th>titleUrl</th>\n",
       "      <th>subtitles</th>\n",
       "      <th>time</th>\n",
       "      <th>products</th>\n",
       "      <th>activityControls</th>\n",
       "      <th>details</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched The Crippled God: Malazan Book of the ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=hOYRKkKs33w</td>\n",
       "      <td>[{'name': 'Oanh Kiều', 'url': 'https://www.you...</td>\n",
       "      <td>2023-05-09T14:30:56.267Z</td>\n",
       "      <td>[YouTube]</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Baby, Please Don't Go</td>\n",
       "      <td>https://www.youtube.com/watch?v=dL60VXOXtSQ</td>\n",
       "      <td>[{'name': 'Them - Topic', 'url': 'https://www....</td>\n",
       "      <td>2023-05-09T14:30:16.025Z</td>\n",
       "      <td>[YouTube]</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Father And Son (2020 Mix)</td>\n",
       "      <td>https://www.youtube.com/watch?v=yfwkUxqBcRw</td>\n",
       "      <td>[{'name': 'Cat Stevens - Topic', 'url': 'https...</td>\n",
       "      <td>2023-05-09T14:26:32.430Z</td>\n",
       "      <td>[YouTube]</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched How to Export/Download Reports from Yo...</td>\n",
       "      <td>https://www.youtube.com/watch?v=-ile8LK5juw</td>\n",
       "      <td>[{'name': 'Analytics Explainer', 'url': 'https...</td>\n",
       "      <td>2023-05-09T11:26:09.595Z</td>\n",
       "      <td>[YouTube]</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched Scrape / Download YouTube Comments and...</td>\n",
       "      <td>https://www.youtube.com/watch?v=uD58-EHwaeI</td>\n",
       "      <td>[{'name': 'Learning Orbis', 'url': 'https://ww...</td>\n",
       "      <td>2023-05-09T09:56:36.726Z</td>\n",
       "      <td>[YouTube]</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          header                                              title  \\\n",
       "0        YouTube  Watched The Crippled God: Malazan Book of the ...   \n",
       "1  YouTube Music                      Watched Baby, Please Don't Go   \n",
       "2  YouTube Music                  Watched Father And Son (2020 Mix)   \n",
       "3        YouTube  Watched How to Export/Download Reports from Yo...   \n",
       "4        YouTube  Watched Scrape / Download YouTube Comments and...   \n",
       "\n",
       "                                      titleUrl  \\\n",
       "0  https://www.youtube.com/watch?v=hOYRKkKs33w   \n",
       "1  https://www.youtube.com/watch?v=dL60VXOXtSQ   \n",
       "2  https://www.youtube.com/watch?v=yfwkUxqBcRw   \n",
       "3  https://www.youtube.com/watch?v=-ile8LK5juw   \n",
       "4  https://www.youtube.com/watch?v=uD58-EHwaeI   \n",
       "\n",
       "                                           subtitles  \\\n",
       "0  [{'name': 'Oanh Kiều', 'url': 'https://www.you...   \n",
       "1  [{'name': 'Them - Topic', 'url': 'https://www....   \n",
       "2  [{'name': 'Cat Stevens - Topic', 'url': 'https...   \n",
       "3  [{'name': 'Analytics Explainer', 'url': 'https...   \n",
       "4  [{'name': 'Learning Orbis', 'url': 'https://ww...   \n",
       "\n",
       "                       time   products         activityControls details  \\\n",
       "0  2023-05-09T14:30:56.267Z  [YouTube]  [YouTube watch history]     NaN   \n",
       "1  2023-05-09T14:30:16.025Z  [YouTube]  [YouTube watch history]     NaN   \n",
       "2  2023-05-09T14:26:32.430Z  [YouTube]  [YouTube watch history]     NaN   \n",
       "3  2023-05-09T11:26:09.595Z  [YouTube]  [YouTube watch history]     NaN   \n",
       "4  2023-05-09T09:56:36.726Z  [YouTube]  [YouTube watch history]     NaN   \n",
       "\n",
       "  description  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_watch_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363aea35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header                  0\n",
       "title                   0\n",
       "titleUrl              769\n",
       "subtitles            4842\n",
       "time                    0\n",
       "products                0\n",
       "activityControls        0\n",
       "details             42033\n",
       "description         42613\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of missing values in each column\n",
    "df_watch_history.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fe05b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43092"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_watch_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85608e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_watch_history.copy()\n",
    "df_copy = df_copy.drop(columns='products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a97e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>title</th>\n",
       "      <th>titleUrl</th>\n",
       "      <th>subtitles</th>\n",
       "      <th>time</th>\n",
       "      <th>activityControls</th>\n",
       "      <th>details</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched The Crippled God: Malazan Book of the ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=hOYRKkKs33w</td>\n",
       "      <td>[{'name': 'Oanh Kiều', 'url': 'https://www.you...</td>\n",
       "      <td>2023-05-09T14:30:56.267Z</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Baby, Please Don't Go</td>\n",
       "      <td>https://www.youtube.com/watch?v=dL60VXOXtSQ</td>\n",
       "      <td>[{'name': 'Them - Topic', 'url': 'https://www....</td>\n",
       "      <td>2023-05-09T14:30:16.025Z</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Father And Son (2020 Mix)</td>\n",
       "      <td>https://www.youtube.com/watch?v=yfwkUxqBcRw</td>\n",
       "      <td>[{'name': 'Cat Stevens - Topic', 'url': 'https...</td>\n",
       "      <td>2023-05-09T14:26:32.430Z</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched How to Export/Download Reports from Yo...</td>\n",
       "      <td>https://www.youtube.com/watch?v=-ile8LK5juw</td>\n",
       "      <td>[{'name': 'Analytics Explainer', 'url': 'https...</td>\n",
       "      <td>2023-05-09T11:26:09.595Z</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched Scrape / Download YouTube Comments and...</td>\n",
       "      <td>https://www.youtube.com/watch?v=uD58-EHwaeI</td>\n",
       "      <td>[{'name': 'Learning Orbis', 'url': 'https://ww...</td>\n",
       "      <td>2023-05-09T09:56:36.726Z</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          header                                              title  \\\n",
       "0        YouTube  Watched The Crippled God: Malazan Book of the ...   \n",
       "1  YouTube Music                      Watched Baby, Please Don't Go   \n",
       "2  YouTube Music                  Watched Father And Son (2020 Mix)   \n",
       "3        YouTube  Watched How to Export/Download Reports from Yo...   \n",
       "4        YouTube  Watched Scrape / Download YouTube Comments and...   \n",
       "\n",
       "                                      titleUrl  \\\n",
       "0  https://www.youtube.com/watch?v=hOYRKkKs33w   \n",
       "1  https://www.youtube.com/watch?v=dL60VXOXtSQ   \n",
       "2  https://www.youtube.com/watch?v=yfwkUxqBcRw   \n",
       "3  https://www.youtube.com/watch?v=-ile8LK5juw   \n",
       "4  https://www.youtube.com/watch?v=uD58-EHwaeI   \n",
       "\n",
       "                                           subtitles  \\\n",
       "0  [{'name': 'Oanh Kiều', 'url': 'https://www.you...   \n",
       "1  [{'name': 'Them - Topic', 'url': 'https://www....   \n",
       "2  [{'name': 'Cat Stevens - Topic', 'url': 'https...   \n",
       "3  [{'name': 'Analytics Explainer', 'url': 'https...   \n",
       "4  [{'name': 'Learning Orbis', 'url': 'https://ww...   \n",
       "\n",
       "                       time         activityControls details description  \n",
       "0  2023-05-09T14:30:56.267Z  [YouTube watch history]     NaN         NaN  \n",
       "1  2023-05-09T14:30:16.025Z  [YouTube watch history]     NaN         NaN  \n",
       "2  2023-05-09T14:26:32.430Z  [YouTube watch history]     NaN         NaN  \n",
       "3  2023-05-09T11:26:09.595Z  [YouTube watch history]     NaN         NaN  \n",
       "4  2023-05-09T09:56:36.726Z  [YouTube watch history]     NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4b4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time is 3457 days, 04:15:41, start date is 2013-11-20 10:15:14.402000+00:00, we have data up to 2023-05-09 14:30:56.267000+00:00\n"
     ]
    }
   ],
   "source": [
    "df_copy['time'] = pd.to_datetime(df_copy['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "time_min = df_copy['time'].min()\n",
    "time_max = df_copy['time'].max()\n",
    "interval = time_max - time_min\n",
    "total_days = interval.days\n",
    "total_seconds = interval.seconds\n",
    "hours, remainder = divmod(total_seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f\"total time is {total_days} days, {hours:02d}:{minutes:02d}:{seconds:02d}, start date is {time_min}, we have data up to {time_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be03b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43092 entries, 0 to 43091\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   header            43092 non-null  object             \n",
      " 1   title             43092 non-null  object             \n",
      " 2   titleUrl          42323 non-null  object             \n",
      " 3   subtitles         38250 non-null  object             \n",
      " 4   time              43092 non-null  datetime64[ns, UTC]\n",
      " 5   activityControls  43092 non-null  object             \n",
      " 6   details           1059 non-null   object             \n",
      " 7   description       479 non-null    object             \n",
      "dtypes: datetime64[ns, UTC](1), object(7)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1703a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header                           object\n",
      "title                            object\n",
      "titleUrl                         object\n",
      "subtitles                        object\n",
      "time                datetime64[ns, UTC]\n",
      "activityControls                 object\n",
      "details                          object\n",
      "description                      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_copy.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e265ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashka\\AppData\\Local\\Temp\\ipykernel_21228\\3662693335.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_copy['title'] = df_copy['title'].str.replace('https://www.youtube.com/watch?', '')\n"
     ]
    }
   ],
   "source": [
    "df_copy['title'] = df_copy['title'].str.replace('https://www.youtube.com/watch?', '')\n",
    "df_copy.drop('titleUrl', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c5c9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitles</th>\n",
       "      <th>time</th>\n",
       "      <th>activityControls</th>\n",
       "      <th>details</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched The Crippled God: Malazan Book of the ...</td>\n",
       "      <td>[{'name': 'Oanh Kiều', 'url': 'https://www.you...</td>\n",
       "      <td>2023-05-09 14:30:56.267000+00:00</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Baby, Please Don't Go</td>\n",
       "      <td>[{'name': 'Them - Topic', 'url': 'https://www....</td>\n",
       "      <td>2023-05-09 14:30:16.025000+00:00</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Father And Son (2020 Mix)</td>\n",
       "      <td>[{'name': 'Cat Stevens - Topic', 'url': 'https...</td>\n",
       "      <td>2023-05-09 14:26:32.430000+00:00</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched How to Export/Download Reports from Yo...</td>\n",
       "      <td>[{'name': 'Analytics Explainer', 'url': 'https...</td>\n",
       "      <td>2023-05-09 11:26:09.595000+00:00</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched Scrape / Download YouTube Comments and...</td>\n",
       "      <td>[{'name': 'Learning Orbis', 'url': 'https://ww...</td>\n",
       "      <td>2023-05-09 09:56:36.726000+00:00</td>\n",
       "      <td>[YouTube watch history]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          header                                              title  \\\n",
       "0        YouTube  Watched The Crippled God: Malazan Book of the ...   \n",
       "1  YouTube Music                      Watched Baby, Please Don't Go   \n",
       "2  YouTube Music                  Watched Father And Son (2020 Mix)   \n",
       "3        YouTube  Watched How to Export/Download Reports from Yo...   \n",
       "4        YouTube  Watched Scrape / Download YouTube Comments and...   \n",
       "\n",
       "                                           subtitles  \\\n",
       "0  [{'name': 'Oanh Kiều', 'url': 'https://www.you...   \n",
       "1  [{'name': 'Them - Topic', 'url': 'https://www....   \n",
       "2  [{'name': 'Cat Stevens - Topic', 'url': 'https...   \n",
       "3  [{'name': 'Analytics Explainer', 'url': 'https...   \n",
       "4  [{'name': 'Learning Orbis', 'url': 'https://ww...   \n",
       "\n",
       "                              time         activityControls details  \\\n",
       "0 2023-05-09 14:30:56.267000+00:00  [YouTube watch history]     NaN   \n",
       "1 2023-05-09 14:30:16.025000+00:00  [YouTube watch history]     NaN   \n",
       "2 2023-05-09 14:26:32.430000+00:00  [YouTube watch history]     NaN   \n",
       "3 2023-05-09 11:26:09.595000+00:00  [YouTube watch history]     NaN   \n",
       "4 2023-05-09 09:56:36.726000+00:00  [YouTube watch history]     NaN   \n",
       "\n",
       "  description  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69963476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header                  0\n",
       "title                   0\n",
       "subtitles            4842\n",
       "time                    0\n",
       "activityControls        0\n",
       "details             42033\n",
       "description         42613\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df1fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('YouTube watch history',)\n",
      " ('Web & App Activity', 'YouTube watch history', 'YouTube search history')\n",
      " ('YouTube watch history', 'Web & App Activity', 'YouTube search history')]\n"
     ]
    }
   ],
   "source": [
    "df_copy['activityControls'] = df_copy['activityControls'].apply(lambda x: tuple(x))\n",
    "print(df_copy['activityControls'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a63da087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('YouTube watch history',)\n",
      " ('Web & App Activity', 'YouTube watch history', 'YouTube search history')\n",
      " ('YouTube watch history', 'Web & App Activity', 'YouTube search history')]\n"
     ]
    }
   ],
   "source": [
    "df_copy['details'] = df_copy['details'].apply(lambda x: tuple(x))\n",
    "print(df_copy['details'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f40ddde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category(title):\n",
    "    category_dict = {\n",
    "        \"Education\": [\"tutorial\", \"how to\", \"lecture\", \"class\", \"educational\", \"science\", \"math\", \"physics\", \"chemistry\", \"biology\", \"history\", \"geography\", \"politics\", \"language\", \"literature\", \"psychology\"],\n",
    "        \"Music\": [\"music\", \"song\", \"album\", \"band\", \"concert\", \"live\"],\n",
    "        \"Comedy\": [\"comedy\", \"funny\", \"joke\", \"laugh\", \"humor\"],\n",
    "        \"News\": [\"news\", \"current events\", \"breaking news\", \"politics\"],\n",
    "        \"Sports\": [\"sports\", \"football\", \"basketball\", \"soccer\", \"tennis\", \"baseball\", \"hockey\", \"golf\"],\n",
    "        \"Gaming\": [\"gaming\", \"gameplay\", \"videogames\", \"esports\", \"live stream\"],\n",
    "        \"Art\": [\"art\", \"painting\", \"drawing\", \"sculpture\", \"design\", \"architecture\", \"fashion\"],\n",
    "        \"Food\": [\"food\", \"cooking\", \"recipe\", \"restaurant\", \"cuisine\", \"baking\"],\n",
    "        \"Travel\": [\"travel\", \"vacation\", \"tourism\", \"adventure\", \"exploration\", \"sightseeing\", \"cultural\"],\n",
    "        \"Technology\": [\"technology\", \"gadgets\", \"electronics\", \"computers\", \"programming\", \"coding\", \"web development\", \"app development\"],\n",
    "        \"Business\": [\"business\", \"entrepreneurship\", \"startups\", \"investing\", \"finance\", \"marketing\", \"sales\"],\n",
    "        \"Health\": [\"health\", \"fitness\", \"nutrition\", \"wellness\", \"medicine\", \"psychology\", \"mindfulness\"],\n",
    "        \"Literature\": [\"book\", \"novel\", \"literature\", \"poetry\", \"reading\", \"writing\", \"author\"],\n",
    "        \"Fantasy/Sci-fi\": [\"fantasy\", \"sci-fi\", \"magic\", \"supernatural\", \"dragons\", \"aliens\", \"robots\"],\n",
    "        \"Audiobook\": [\"audiobook\", \"audio book\", \"audiobook full\"],\n",
    "        \"Beauty\": [\"beauty\", \"makeup\", \"skincare\", \"haircare\"],\n",
    "        \"DIY\": [\"diy\", \"do it yourself\", \"craft\", \"handmade\", \"home improvement\", \"woodworking\", \"gardening\"],\n",
    "        \"Animals\": [\"animals\", \"pets\", \"wildlife\", \"nature\", \"animal rescue\"],\n",
    "        \"Social Issues\": [\"social issues\", \"activism\", \"equality\", \"diversity\", \"inclusion\"],\n",
    "        \"Religion/Spirituality\": [\"religion\", \"spirituality\", \"faith\", \"belief\", \"prayer\", \"meditation\"],\n",
    "        \"Psychology\": [\"psychology\", \"mental health\", \"mind\", \"behavior\", \"emotions\", \"cognition\", \"therapy\", \"counseling\", \"psychiatry\", \"self-help\", \"motivation\", \"personality\"],\n",
    "    }\n",
    "\n",
    "    for category, keywords in category_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in title.lower():\n",
    "                return category\n",
    "    \n",
    "    return \"Other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c761e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['category'] = df_copy['title'].apply(extract_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02f98f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitles</th>\n",
       "      <th>time</th>\n",
       "      <th>activityControls</th>\n",
       "      <th>details</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched The Crippled God: Malazan Book of the ...</td>\n",
       "      <td>[{'name': 'Oanh Kiều', 'url': 'https://www.you...</td>\n",
       "      <td>2023-05-09 14:30:56.267000+00:00</td>\n",
       "      <td>(YouTube watch history,)</td>\n",
       "      <td>()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Baby, Please Don't Go</td>\n",
       "      <td>[{'name': 'Them - Topic', 'url': 'https://www....</td>\n",
       "      <td>2023-05-09 14:30:16.025000+00:00</td>\n",
       "      <td>(YouTube watch history,)</td>\n",
       "      <td>()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YouTube Music</td>\n",
       "      <td>Watched Father And Son (2020 Mix)</td>\n",
       "      <td>[{'name': 'Cat Stevens - Topic', 'url': 'https...</td>\n",
       "      <td>2023-05-09 14:26:32.430000+00:00</td>\n",
       "      <td>(YouTube watch history,)</td>\n",
       "      <td>()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched How to Export/Download Reports from Yo...</td>\n",
       "      <td>[{'name': 'Analytics Explainer', 'url': 'https...</td>\n",
       "      <td>2023-05-09 11:26:09.595000+00:00</td>\n",
       "      <td>(YouTube watch history,)</td>\n",
       "      <td>()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YouTube</td>\n",
       "      <td>Watched Scrape / Download YouTube Comments and...</td>\n",
       "      <td>[{'name': 'Learning Orbis', 'url': 'https://ww...</td>\n",
       "      <td>2023-05-09 09:56:36.726000+00:00</td>\n",
       "      <td>(YouTube watch history,)</td>\n",
       "      <td>()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          header                                              title  \\\n",
       "0        YouTube  Watched The Crippled God: Malazan Book of the ...   \n",
       "1  YouTube Music                      Watched Baby, Please Don't Go   \n",
       "2  YouTube Music                  Watched Father And Son (2020 Mix)   \n",
       "3        YouTube  Watched How to Export/Download Reports from Yo...   \n",
       "4        YouTube  Watched Scrape / Download YouTube Comments and...   \n",
       "\n",
       "                                           subtitles  \\\n",
       "0  [{'name': 'Oanh Kiều', 'url': 'https://www.you...   \n",
       "1  [{'name': 'Them - Topic', 'url': 'https://www....   \n",
       "2  [{'name': 'Cat Stevens - Topic', 'url': 'https...   \n",
       "3  [{'name': 'Analytics Explainer', 'url': 'https...   \n",
       "4  [{'name': 'Learning Orbis', 'url': 'https://ww...   \n",
       "\n",
       "                              time          activityControls details  \\\n",
       "0 2023-05-09 14:30:56.267000+00:00  (YouTube watch history,)      ()   \n",
       "1 2023-05-09 14:30:16.025000+00:00  (YouTube watch history,)      ()   \n",
       "2 2023-05-09 14:26:32.430000+00:00  (YouTube watch history,)      ()   \n",
       "3 2023-05-09 11:26:09.595000+00:00  (YouTube watch history,)      ()   \n",
       "4 2023-05-09 09:56:36.726000+00:00  (YouTube watch history,)      ()   \n",
       "\n",
       "  description   category  \n",
       "0         NaN      Other  \n",
       "1         NaN      Other  \n",
       "2         NaN      Other  \n",
       "3         NaN  Education  \n",
       "4         NaN      Other  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ae82e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other                    33806\n",
      "Music                     2250\n",
      "Education                 2123\n",
      "Art                       1693\n",
      "Comedy                     804\n",
      "Psychology                 343\n",
      "News                       290\n",
      "Sports                     288\n",
      "Food                       253\n",
      "Literature                 241\n",
      "DIY                        228\n",
      "Business                   142\n",
      "Fantasy/Sci-fi             130\n",
      "Animals                     98\n",
      "Health                      84\n",
      "Gaming                      78\n",
      "Technology                  71\n",
      "Travel                      69\n",
      "Religion/Spirituality       63\n",
      "Social Issues               23\n",
      "Beauty                      15\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "category_counts = df_copy['category'].value_counts()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "990252c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shorts: 7967\n",
      "video: 1778\n",
      "joe: 1043\n",
      "jordan: 1014\n",
      "wow: 937\n",
      "official: 916\n",
      "music: 878\n",
      "peterson: 836\n",
      "rogan: 823\n",
      "live: 809\n",
      "v: 796\n",
      "vs: 794\n",
      "trump: 789\n",
      "removed: 673\n",
      "best: 653\n",
      "part: 624\n",
      "man: 596\n",
      "world: 575\n",
      "show: 543\n",
      "time: 521\n",
      "new: 505\n",
      "shadowlands: 484\n",
      "people: 471\n",
      "one: 459\n",
      "like: 444\n",
      "battle: 440\n",
      "dont: 423\n",
      "andrew: 423\n",
      "life: 410\n",
      "know: 407\n",
      "english: 405\n",
      "guide: 397\n",
      "pvp: 396\n",
      "make: 395\n",
      "book: 394\n",
      "funny: 389\n",
      "duels: 377\n",
      "ever: 373\n",
      "game: 370\n",
      "azeroth: 370\n",
      "get: 365\n",
      "first: 353\n",
      "tate: 352\n",
      "top: 350\n",
      "daily: 347\n",
      "day: 344\n",
      "viral: 336\n",
      "joerogan: 332\n",
      "got: 330\n",
      "friends: 328\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove punctuation and digits from titles\n",
    "df_copy['title'] = df_copy['title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df_copy['title'] = df_copy['title'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove stop words and tokenize the titles\n",
    "tokens = []\n",
    "for title in df_copy['title']:\n",
    "    title_tokens = title.lower().split()\n",
    "    title_tokens = [token for token in title_tokens if token not in stop_words]\n",
    "    tokens += title_tokens\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_count = Counter(tokens)\n",
    "\n",
    "# Sort the words by their frequency in descending order\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 50 words\n",
    "for word, count in sorted_words[:50]:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0dec38cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: stop_words\n",
      "  Building wheel for stop_words (setup.py): started\n",
      "  Building wheel for stop_words (setup.py): finished with status 'done'\n",
      "  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32893 sha256=4a060fe3bfaae702f51e042c7d4c6d901eda8ef91e045f34bf982597fb0c41a0\n",
      "  Stored in directory: c:\\users\\ashka\\appdata\\local\\pip\\cache\\wheels\\da\\d8\\66\\395317506a23a9d1d7de433ad6a7d9e6e16aab48cf028a0f60\n",
      "Successfully built stop_words\n",
      "Installing collected packages: stop_words\n",
      "Successfully installed stop_words-2018.7.23\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bba06e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9540e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short: 8216\n",
      "video: 1812\n",
      "joe: 1043\n",
      "jordan: 1032\n",
      "trump: 990\n",
      "wow: 941\n",
      "official: 922\n",
      "music: 878\n",
      "peterson: 836\n",
      "rogan: 823\n",
      "live: 809\n",
      "world: 685\n",
      "man: 673\n",
      "removed: 673\n",
      "best: 653\n",
      "part: 636\n",
      "get: 635\n",
      "time: 616\n",
      "show: 595\n",
      "make: 591\n",
      "one: 532\n",
      "new: 505\n",
      "people: 487\n",
      "shadowlands: 484\n",
      "know: 459\n",
      "like: 456\n",
      "battle: 446\n",
      "life: 440\n",
      "andrew: 430\n",
      "dont: 423\n",
      "book: 418\n",
      "game: 407\n",
      "english: 405\n",
      "tate: 401\n",
      "day: 400\n",
      "guide: 399\n",
      "friend: 396\n",
      "pvp: 396\n",
      "funny: 389\n",
      "duel: 389\n",
      "ever: 373\n",
      "azeroth: 370\n",
      "first: 353\n",
      "top: 351\n",
      "daily: 349\n",
      "woman: 346\n",
      "thing: 337\n",
      "year: 336\n",
      "viral: 336\n",
      "joerogan: 332\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import stop_words\n",
    "\n",
    "stop_words = set(stopwords.words('english')) | set(stop_words.get_stop_words('en'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define regular expressions for filtering out words\n",
    "regex_remove = re.compile(r'(^[a-z]{1,2}$)|(^[^\\w]+$)|(\\d+)')\n",
    "regex_replace = re.compile(r'([^\\w\\s]+)')\n",
    "\n",
    "# Remove stop words, lemmatize, and tokenize the titles\n",
    "tokens = []\n",
    "for title in df_copy['title']:\n",
    "    title_tokens = regex_replace.sub(' ', title.lower())\n",
    "    title_tokens = title_tokens.split()\n",
    "    title_tokens = [lemmatizer.lemmatize(token) for token in title_tokens if not regex_remove.match(token) and token not in stop_words]\n",
    "    tokens += title_tokens\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_count = Counter(tokens)\n",
    "\n",
    "# Sort the words by their frequency in descending order\n",
    "sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 50 words\n",
    "for word, count in sorted_words[:50]:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3b0ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['show', 'daily', 'graham', 'norton', 'trump', 'youtube', 'noah', 'change', 'music', 'visited']\n",
      "\n",
      "Topic 1:\n",
      "['short', 'man', 'elon', 'musk', 'bill', 'real', 'math', 'war', 'tyson', 'time']\n",
      "\n",
      "Topic 2:\n",
      "['live', 'intro', 'year', 'statistic', 'people', 'rich', 'حسن', 'hasan', 'آقامیری', 'aghamiri']\n",
      "\n",
      "Topic 3:\n",
      "['short', 'joe', 'rogan', 'tate', 'andrew', 'joerogan', 'live', 'story', 'sting', 'like']\n",
      "\n",
      "Topic 4:\n",
      "['guide', 'official', 'short', 'bad', 'remaster', 'nick', 'cave', 'video', 'world', 'seed']\n",
      "\n",
      "Topic 5:\n",
      "['wow', 'part', 'battle', 'book', 'shadowlands', 'pvp', 'duel', 'azeroth', 'short', 'patch']\n",
      "\n",
      "Topic 6:\n",
      "['short', 'video', 'official', 'removed', 'music', 'ben', 'kobe', 'dave', 'man', 'nba']\n",
      "\n",
      "Topic 7:\n",
      "['short', 'jordan', 'peterson', 'woman', 'get', 'ever', 'funny', 'thing', 'comedy', 'men']\n",
      "\n",
      "Topic 8:\n",
      "['short', 'trump', 'work', 'top', 'tank', 'ricky', 'gervais', 'shark', 'tom', 'day']\n",
      "\n",
      "Topic 9:\n",
      "['trump', 'new', 'viking', 'live', 'msnbc', 'make', 'first', 'mueller', 'game', 'report']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashka\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define regular expressions for filtering out words\n",
    "regex_remove = re.compile(r'(^[a-z]{1,2}$)|(^[^\\w]+$)|(\\d+)')\n",
    "regex_replace = re.compile(r'([^\\w\\s]+)')\n",
    "\n",
    "# Remove stop words, lemmatize, and tokenize the titles\n",
    "df_copy['title_tokens'] = df_copy['title'].apply(lambda x: regex_replace.sub(' ', x.lower()))\n",
    "df_copy['title_tokens'] = df_copy['title_tokens'].apply(lambda x: x.split())\n",
    "df_copy['title_tokens'] = df_copy['title_tokens'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x if not regex_remove.match(token) and token not in stop_words])\n",
    "df_copy['title_tokens'] = df_copy['title_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Vectorize the titles\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_copy['title_tokens'])\n",
    "\n",
    "# Fit LDA model\n",
    "n_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Print the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    top_words_idx = topic.argsort()[:-11:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(top_words)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d4b7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['show', 'graham', 'change', 'norton', 'daily', 'getting', 'always', 'career', 'dating', 'asked']\n",
      "\n",
      "Topic 1:\n",
      "['time', 'real', 'tyson', 'mike', 'neil', 'degrasse', 'rule', 'bill', 'maher', 'short']\n",
      "\n",
      "Topic 2:\n",
      "['year', 'old', 'water', 'tiktok', 'با', 'form', 'قسمت', 'xamarin', 'deep', 'prison']\n",
      "\n",
      "Topic 3:\n",
      "['tate', 'andrew', 'short', 'ricky', 'gervais', 'win', 'roast', 'tristan', 'sander', 'bernie']\n",
      "\n",
      "Topic 4:\n",
      "['explains', 'look', 'easy', 'remaster', 'space', 'drug', 'mind', 'connect', 'wild', 'working']\n",
      "\n",
      "Topic 5:\n",
      "['top', 'interview', 'song', 'question', 'rock', 'answer', 'snl', 'insane', 'ielts', 'part']\n",
      "\n",
      "Topic 6:\n",
      "['short', 'ben', 'shapiro', 'stop', 'night', 'james', 'david', 'caught', 'lebron', 'debate']\n",
      "\n",
      "Topic 7:\n",
      "['got', 'short', 'mark', 'lee', 'destroys', 'almost', 'get', 'read', 'cuban', 'well']\n",
      "\n",
      "Topic 8:\n",
      "['trump', 'show', 'daily', 'tank', 'donald', 'noah', 'trevor', 'shark', 'ukraine', 'he']\n",
      "\n",
      "Topic 9:\n",
      "['one', 'first', 'bad', 'breaking', 'short', 'power', 'here', 'person', 'dragon', 'sepulcher']\n",
      "\n",
      "Topic 10:\n",
      "['new', 'gold', 'million', 'perfect', 'making', 'king', 'science', 'johnson', 'ice', 'sweden']\n",
      "\n",
      "Topic 11:\n",
      "['viking', 'short', 'dog', 'tip', 'food', 'ragnar', 'hand', 'amazing', 'everyone', 'buy']\n",
      "\n",
      "Topic 12:\n",
      "['fight', 'kid', 'car', 'dead', 'ufc', 'south', 'everything', 'fighter', 'short', 'mma']\n",
      "\n",
      "Topic 13:\n",
      "['حسن', 'hasan', 'آقامیری', 'aghamiri', 'every', 'come', 'fear', 'true', 'reading', 'wise']\n",
      "\n",
      "Topic 14:\n",
      "['part', 'book', 'fallen', 'malazan', 'steven', 'erikson', 'remastered', 'animal', 'moon', 'concert']\n",
      "\n",
      "Topic 15:\n",
      "['wow', 'battle', 'pvp', 'azeroth', 'duel', 'guide', 'trick', 'paladin', 'end', 'actually']\n",
      "\n",
      "Topic 16:\n",
      "['dont', 'know', 'short', 'thing', 'war', 'think', 'didnt', 'play', 'gun', 'military']\n",
      "\n",
      "Topic 17:\n",
      "['short', 'joke', 'dad', 'set', 'run', 'future', 'hate', 'talking', 'warlock', 'jack']\n",
      "\n",
      "Topic 18:\n",
      "['short', 'cant', 'big', 'wife', 'went', 'russia', 'ultimate', 'funniest', 'speed', 'party']\n",
      "\n",
      "Topic 19:\n",
      "['black', 'youtube', 'kevin', 'hart', 'visited', 'try', 'music', 'school', 'version', 'student']\n",
      "\n",
      "Topic 20:\n",
      "['deal', 'still', 'found', 'steve', 'son', 'family', 'state', 'diamond', 'cat', 'father']\n",
      "\n",
      "Topic 21:\n",
      "['short', 'bill', 'great', 'burr', 'blue', 'stand', 'laugh', 'sport', 'hero', 'comedy']\n",
      "\n",
      "Topic 22:\n",
      "['man', 'short', 'woman', 'news', 'men', 'white', 'biggest', 'satisfying', 'respect', 'young']\n",
      "\n",
      "Topic 23:\n",
      "['job', 'home', 'short', 'start', 'tutorial', 'thought', 'line', 'long', 'build', 'lesson']\n",
      "\n",
      "Topic 24:\n",
      "['people', 'want', 'rich', 'really', 'give', 'short', 'pizza', 'get', 'keep', 'suit']\n",
      "\n",
      "Topic 25:\n",
      "['never', 'short', 'challenge', 'girl', 'right', 'america', 'let', 'johnny', 'mage', 'jam']\n",
      "\n",
      "Topic 26:\n",
      "['back', 'heart', 'child', 'jake', 'jim', 'list', 'wanted', 'paul', 'اخبار', 'wish']\n",
      "\n",
      "Topic 27:\n",
      "['secret', 'watch', 'youre', 'peter', 'mrbeast', 'dream', 'dark', 'test', 'without', 'short']\n",
      "\n",
      "Topic 28:\n",
      "['world', 'moment', 'warcraft', 'pov', 'football', 'highlight', 'unholy', 'mythic', 'cup', 'lecture']\n",
      "\n",
      "Topic 29:\n",
      "['short', 'math', 'problem', 'drdisrespect', 'motivation', 'break', 'messi', 'gordon', 'cop', 'side']\n",
      "\n",
      "Topic 30:\n",
      "['short', 'movie', 'shaq', 'hit', 'peaky', 'blinder', 'beast', 'shelby', 'season', 'soldier']\n",
      "\n",
      "Topic 31:\n",
      "['shadowlands', 'patch', 'wow', 'need', 'american', 'death', 'guide', 'livestream', 'raid', 'feel']\n",
      "\n",
      "Topic 32:\n",
      "['msnbc', 'trump', 'house', 'president', 'chris', 'morning', 'stephen', 'live', 'impeachment', 'go']\n",
      "\n",
      "Topic 33:\n",
      "['day', 'last', 'در', 'john', 'week', 'fact', 'از', 'ایران', 'left', 'save']\n",
      "\n",
      "Topic 34:\n",
      "['short', 'country', 'tom', 'comedy', 'put', 'voice', 'city', 'place', 'cruise', 'gift']\n",
      "\n",
      "Topic 35:\n",
      "['joe', 'rogan', 'short', 'joerogan', 'story', 'crazy', 'podcast', 'dave', 'jre', 'chappelle']\n",
      "\n",
      "Topic 36:\n",
      "['english', 'learn', 'music', 'swedish', 'language', 'minute', 'hour', 'sleep', 'live', 'british']\n",
      "\n",
      "Topic 37:\n",
      "['wrong', 'whats', 'orchestra', 'huberman', 'andrew', 'manchester', 'simple', 'hell', 'said', 'skill']\n",
      "\n",
      "Topic 38:\n",
      "['elon', 'money', 'short', 'musk', 'word', 'free', 'see', 'use', 'much', 'make']\n",
      "\n",
      "Topic 39:\n",
      "['short', 'ever', 'michael', 'good', 'nba', 'made', 'kobe', 'advice', 'player', 'bryant']\n",
      "\n",
      "Topic 40:\n",
      "['intro', 'statistic', 'jimmy', 'pay', 'solution', 'bos', 'adam', 'experience', 'millionaire', 'oddlysatisfying']\n",
      "\n",
      "Topic 41:\n",
      "['lyric', 'stone', 'nick', 'cave', 'live', 'seed', 'rolling', 'bad', 'baby', 'around']\n",
      "\n",
      "Topic 42:\n",
      "['best', 'way', 'name', 'short', 'class', 'clip', 'level', 'teach', 'someone', 'god']\n",
      "\n",
      "Topic 43:\n",
      "['would', 'going', 'short', 'two', 'truth', 'russian', 'even', 'idea', 'behind', 'billionaire']\n",
      "\n",
      "Topic 44:\n",
      "['jordan', 'peterson', 'make', 'short', 'speech', 'sense', 'french', 'wardruna', 'jordanpeterson', 'motivation']\n",
      "\n",
      "Topic 45:\n",
      "['short', 'boy', 'doesnt', 'different', 'using', 'khabib', 'wait', 'scary', 'pro', 'update']\n",
      "\n",
      "Topic 46:\n",
      "['short', 'viral', 'love', 'work', 'better', 'fyp', 'call', 'soprano', 'mueller', 'report']\n",
      "\n",
      "Topic 47:\n",
      "['video', 'official', 'removed', 'music', 'eminem', 'cooper', 'cent', 'college', 'rap', 'anderson']\n",
      "\n",
      "Topic 48:\n",
      "['game', 'throne', 'steak', 'fire', 'short', 'final', 'paul', 'second', 'beef', 'gameofthrones']\n",
      "\n",
      "Topic 49:\n",
      "['friend', 'funny', 'short', 'joey', 'little', 'help', 'diaz', 'arena', 'brother', 'ryan']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define regular expressions for filtering out words\n",
    "regex_remove = re.compile(r'(^[a-z]{1,2}$)|(^[^\\w]+$)|(\\d+)')\n",
    "regex_replace = re.compile(r'([^\\w\\s]+)')\n",
    "\n",
    "# Remove stop words, lemmatize, and tokenize the titles\n",
    "df_copy['title_tokens'] = df_copy['title'].apply(lambda x: regex_replace.sub(' ', x.lower()))\n",
    "df_copy['title_tokens'] = df_copy['title_tokens'].apply(lambda x: x.split())\n",
    "df_copy['title_tokens'] = df_copy['title_tokens'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x if not regex_remove.match(token) and token not in stop_words])\n",
    "df_copy['title_tokens'] = df_copy['title_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Vectorize the titles\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_copy['title_tokens'])\n",
    "\n",
    "# Fit LDA model\n",
    "n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=1000, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Print the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    top_words_idx = topic.argsort()[:-11:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(top_words)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350c5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
